This run was done on: Sun Aug 24 14:04:43 SGT 2014
HPL.dat: 
HPLinpack benchmark input file
Innovative Computing Laboratory, University of Tennessee
HPL.out      output file name (if any)
6            device out (6=stdout,7=stderr,file)
2            # of problems sizes (N)
1000 2000    Ns
1            # of NBs
168          NBs
0            PMAP process mapping (0=Row-,1=Column-major)
1            # of process grids (P x Q)
1            Ps
4            Qs
16.0         threshold
1            # of panel fact
2            PFACTs (0=left, 1=Crout, 2=Right)
1            # of recursive stopping criterium
4            NBMINs (>= 1)
1            # of panels in recursion
2            NDIVs
1            # of recursive panel fact.
1            RFACTs (0=left, 1=Crout, 2=Right)
1            # of broadcast
1            BCASTs (0=1rg,1=1rM,2=2rg,3=2rM,4=Lng,5=LnM)
1            # of lookahead depth
0            DEPTHs (>=0)
2            SWAP (0=bin-exch,1=long,2=mix)
64           swapping threshold
0            L1 in (0=transposed,1=no-transposed) form
0            U  in (0=transposed,1=no-transposed) form
1            Equilibration (0=no,1=yes)
8            memory alignment in double (> 0)
Binary name: 
-rwxr-xr-x 1 student11 student2014 7866401 Apr 28 16:41 xhpl_intel64
This script: 
#!/bin/bash
#
echo "This is a SAMPLE run script.  Change it to reflect the correct number"
echo "of CPUs/threads, number of nodes, MPI processes per node, etc.."
#

#
# You can find description of all Intel MPI parameters in the
# Intel MPI Reference Manual.
# See <intel mpi installdir>/doc/Reference_manual.pdf
#

export I_MPI_EAGER_THRESHOLD=128000
#          This setting may give 1-2% of performance increase over the
#          default value of 262000 for large problems and high number of cores

export OUT=xhpl_intel64_outputs.txt

date

cp HPL_serial.dat HPL.dat

echo -n "This run was done on: "
date

# Capture some meaningful data for future reference:
echo -n "This run was done on: " >> $OUT
date >> $OUT
echo "HPL.dat: " >> $OUT
cat HPL.dat >> $OUT
echo "Binary name: " >> $OUT
ls -l xhpl_intel64 >> $OUT
echo "This script: " >> $OUT
cat runme_intel64 >> $OUT
echo "Environment variables: " >> $OUT
env >> $OUT
echo "Actual run: " >> $OUT

# Environment variables can also be set on the Intel MPI command
# line using the -genv option:

mpiexec -np 4 ./xhpl_intel64 | tee -a xhpl_intel64_outputs.txt

# In case of multiple nodes involved, please set the number of MPI processes
# per node (ppn=1,2 typically) through the -perhost option (because the
# default is all cores):

# mpiexec -perhost <ppn> -np <n> ./xhpl_intel64

echo -n "Done: " >> $OUT
date >> $OUT

echo -n "Done: "
date
Environment variables: 
MKLROOT=/apps/intel/composer_xe_2013_sp1.2.144/mkl
MANPATH=/apps/intel/impi/4.1.3.048/man:/apps/intel/composer_xe_2013_sp1.2.144/man/en_US:/apps/gcc/4.8.3/share/man:/usr/share/man
HOSTNAME=cpunode0-ulam.localdomain
INTEL_LICENSE_FILE=/apps/intel/licenses
IPPROOT=/apps/intel/composer_xe_2013_sp1.2.144/ipp
SHELL=/bin/bash
TERM=xterm
HISTSIZE=1000
SSH_CLIENT=49.245.42.91 61894 22
GDBSERVER_MIC=/apps/intel/composer_xe_2013_sp1.2.144/debugger/gdb/target/mic/bin/gdbserver
LIBRARY_PATH=/apps/intel/composer_xe_2013_sp1.2.144/compiler/lib/intel64:/apps/intel/composer_xe_2013_sp1.2.144/ipp/../compiler/lib/intel64:/apps/intel/composer_xe_2013_sp1.2.144/ipp/lib/intel64:/apps/intel/composer_xe_2013_sp1.2.144/mkl/lib/intel64:/apps/intel/composer_xe_2013_sp1.2.144/tbb/lib/intel64/gcc4.4:/usr/lib64/nvidia:/apps/gcc/4.8.3/lib64
OUT=xhpl_intel64_outputs.txt
SSH_TTY=/dev/pts/2
MIC_LD_LIBRARY_PATH=/apps/intel/composer_xe_2013_sp1.2.144/compiler/lib/mic:/apps/intel/composer_xe_2013_sp1.2.144/mpirt/lib/mic:/apps/intel/composer_xe_2013_sp1.2.144/mkl/lib/mic:/apps/intel/composer_xe_2013_sp1.2.144/tbb/lib/mic
USER=student11
LS_COLORS=rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=01;05;37;41:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arj=01;31:*.taz=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lz=01;31:*.xz=01;31:*.bz2=01;31:*.tbz=01;31:*.tbz2=01;31:*.bz=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.rar=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.jpg=01;35:*.jpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.axv=01;35:*.anx=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=01;36:*.au=01;36:*.flac=01;36:*.mid=01;36:*.midi=01;36:*.mka=01;36:*.mp3=01;36:*.mpc=01;36:*.ogg=01;36:*.ra=01;36:*.wav=01;36:*.axa=01;36:*.oga=01;36:*.spx=01;36:*.xspf=01;36:
LD_LIBRARY_PATH=.:/apps/intel/impi/4.1.3.048/intel64/lib:/apps/intel/composer_xe_2013_sp1.2.144/compiler/lib/intel64:/apps/intel/composer_xe_2013_sp1.2.144/mpirt/lib/intel64:/apps/intel/composer_xe_2013_sp1.2.144/ipp/../compiler/lib/intel64:/apps/intel/composer_xe_2013_sp1.2.144/ipp/lib/intel64:/apps/intel/composer_xe_2013_sp1.2.144/mkl/lib/intel64:/apps/intel/composer_xe_2013_sp1.2.144/tbb/lib/intel64/gcc4.4:/apps/cuda/6.0/lib64:/apps/gcc/4.8.3/lib64
MIC_LIBRARY_PATH=/apps/intel/composer_xe_2013_sp1.2.144/tbb/lib/mic
CPATH=/apps/intel/composer_xe_2013_sp1.2.144/mkl/include:/apps/intel/composer_xe_2013_sp1.2.144/tbb/include
NLSPATH=/apps/intel/composer_xe_2013_sp1.2.144/compiler/lib/intel64/locale/%l_%t/%N:/apps/intel/composer_xe_2013_sp1.2.144/ipp/lib/intel64/locale/%l_%t/%N:/apps/intel/composer_xe_2013_sp1.2.144/mkl/lib/intel64/locale/%l_%t/%N:/apps/intel/composer_xe_2013_sp1.2.144/debugger/gdb/intel64_mic/py26/share/locale/%l_%t/%N:/apps/intel/composer_xe_2013_sp1.2.144/debugger/gdb/intel64/py26/share/locale/%l_%t/%N:/apps/intel/composer_xe_2013_sp1.2.144/debugger/intel64/locale/%l_%t/%N
I_MPI_EAGER_THRESHOLD=128000
PATH=/apps/intel/impi/4.1.3.048/intel64/bin:/apps/intel/composer_xe_2013_sp1.2.144/bin/intel64:/apps/intel/composer_xe_2013_sp1.2.144/mpirt/bin/intel64:/apps/intel/composer_xe_2013_sp1.2.144/debugger/gdb/intel64_mic/py26/bin:/apps/intel/composer_xe_2013_sp1.2.144/debugger/gdb/intel64/py26/bin:/apps/intel/composer_xe_2013_sp1.2.144/bin/intel64_mic:/apps/intel/composer_xe_2013_sp1.2.144/debugger/gui/intel64:/apps/cuda/6.0/bin:/apps/gcc/4.8.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/student11/bin
MAIL=/var/spool/mail/student11
TBBROOT=/apps/intel/composer_xe_2013_sp1.2.144/tbb
PWD=/home/student11/scratch/linpack/linpack_11.1.3/benchmarks/mp_linpack/bin_intel/intel64
_LMFILES_=/apps/modules/gcc/4.8.3:/apps/modules/cuda/6.0:/apps/modules/intel/composer-xe-2013/14.0.2:/apps/modules/intel/mpi/4.1.3
IDB_HOME=/apps/intel/composer_xe_2013_sp1.2.144/bin/intel64
LANG=en_US.UTF-8
GDB_CROSS=/apps/intel/composer_xe_2013_sp1.2.144/debugger/gdb/intel64_mic/py26/bin/gdb-mic
MODULEPATH=/apps/modules:/home/sc14demo/common-apps/modulefiles
LOADEDMODULES=gcc/4.8.3:cuda/6.0:intel/composer-xe-2013/14.0.2:intel/mpi/4.1.3
HISTCONTROL=ignoredups
HOME=/home/student11
SHLVL=2
LOGNAME=student11
SSH_CONNECTION=49.245.42.91 61894 202.83.248.74 22
MODULESHOME=/usr/share/Modules
LESSOPEN=|/usr/bin/lesspipe.sh %s
INCLUDE=/apps/intel/composer_xe_2013_sp1.2.144/mkl/include
G_BROKEN_FILENAMES=1
I_MPI_ROOT=/apps/intel/impi/4.1.3.048
module=() {  eval `/usr/bin/modulecmd bash $*`
}
_=/bin/env
Actual run: 
mpiexec_cpunode0-ulam.localdomain: cannot connect to local mpd (/tmp/mpd2.console_student11); possible causes:
  1. no mpd is running on this host
  2. an mpd is running but was started without a "console" (-n option)
Done: Sun Aug 24 14:04:43 SGT 2014
