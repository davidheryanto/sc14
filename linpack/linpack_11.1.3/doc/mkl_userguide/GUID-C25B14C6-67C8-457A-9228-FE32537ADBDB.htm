<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<!-- saved from url=(0014)about:internet -->
<html xmlns:MSHelp="http://www.microsoft.com/MSHelp/" lang="en-us" xml:lang="en-us"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<meta name="DC.Type" content="topic">
<meta name="DC.Title" content="Heterogeneous Intel Optimized MP LINPACK Benchmark">
<meta name="DC.subject" content="Intel&reg; Optimized MP LINPACK Benchmark for Clusters, inter-node and intra-node heterogeneity, heterogeneous cluster, support by Intel&reg; Optimized MP LINPACK Benchmark for Clusters">
<meta name="keywords" content="Intel&reg; Optimized MP LINPACK Benchmark for Clusters, inter-node and intra-node heterogeneity, heterogeneous cluster, support by Intel&reg; Optimized MP LINPACK Benchmark for Clusters">
<meta name="DC.Relation" scheme="URI" content="GUID-10DFCB17-3953-47C6-9971-8C455A925BFE.htm">
<meta name="DC.Relation" scheme="URI" content="http://www.intel.com/software/products/softwaredocs_feedback">
<meta name="DC.Format" content="XHTML">
<meta name="DC.Identifier" content="GUID-C25B14C6-67C8-457A-9228-FE32537ADBDB">
<meta name="DC.Language" content="en-US">
<link rel="stylesheet" type="text/css" href="intel_css_styles.css">
<title>Heterogeneous Intel Optimized MP LINPACK Benchmark</title>
<xml>
<MSHelp:Attr Name="DocSet" Value="Intel"></MSHelp:Attr>
<MSHelp:Attr Name="Locale" Value="kbEnglish"></MSHelp:Attr>
<MSHelp:Attr Name="TopicType" Value="kbReference"></MSHelp:Attr>
</xml>
</head>
<body id="GUID-C25B14C6-67C8-457A-9228-FE32537ADBDB">
 <!-- ==============(Start:NavScript)================= -->
 <script src="NavScript.js" language="JavaScript1.2" type="text/javascript"></script>
 <script language="JavaScript1.2" type="text/javascript">WriteNavLink(0);</script>
 <!-- ==============(End:NavScript)================= -->
<p id="header_text" style="margin-bottom : 20pt"><em>Intel&reg; Math Kernel Library 11.1 Update 3 User's Guide</em></p>


 
  <h1 class="topictitle1">Heterogeneous Intel Optimized MP LINPACK Benchmark</h1>
 
   
  <div id="GUID-7F5E96E6-9067-4129-B32F-D072D84A40C3"> 
    <p>The Intel Optimized MP LINPACK Benchmark supports both inter-node and intra-node heterogeneity. Only the hybrid offload version of the prebuilt binaries supports node-to-node heterogeneity. 
    </p>
 
    <p>Intel MKL achieves heterogeneous support by distributing the matrix data unequally between the nodes. You do not need to run a different number of MPI processes on the faster nodes. Heterogeneous support works by running only one MPI process per node and assigning more work to the more powerful nodes. The heterogeneous factor command-line parameter 
      <samp class="codeph">f</samp> controls the amount of work to be assigned to the more powerful nodes: 
    </p>
 
    <p><samp class="codeph"> 
        ./xhpl –n 100000 –b 1024 –p 8 –q 16 –f 
        <var>&lt;heterogeneous factor&gt;</var></samp> 
    </p>
 
    <p>The heterogeneous factor does not need to be integer. If the heterogeneous factor is 2.5, roughly 2.5 times the work will be put on the more powerful nodes. The heterogeneous factor is an important tuning parameter. The more work you put on the more powerful nodes, the more memory you might be wasting on the other nodes. Intel MKL achieves load balance by distributing the work unevenly. You can balance for speed or balance for memory, but not both. If your cluster includes many different types of nodes, you may need multiple heterogeneous factors.  
    </p>
 
    <p>Suppose you have a cluster with 32 GB per node, but some of the nodes are twice as fast, so you put twice as much work on them. Because they only have 32 GB per node as well, all the other nodes must be treated as 16 GB nodes, and effectively waste half the memory. Note that wasting memory might require a smaller problem size, which lowers the overall performance. 
    </p>
 
    <p>Let 
      <var>P</var> be the number of rows and 
      <var>Q</var> the number of columns in your processor grid (<var>PxQ</var>). The work must be 
      <em>homogeneous</em> within each processor column because vertical operations, such as pivoting, are synchronizing operations. 
    </p>
 
    <p>When there are two different types of nodes, use MPI to process all the faster nodes first, and make sure the "<samp class="codeph">PMAP process mapping</samp>" (line 9) of 
      <span class="filepath">HPL.dat</span> is set to 1, for 
      <samp class="codeph">Column-major</samp> mapping. Because all the nodes must be the same within a process column, the number of faster nodes must always be a multiple of 
      <var>P</var>, and you can specify the faster nodes by setting the number of process columns 
      <var>C</var> for the faster nodes with the 
      <samp class="codeph">c</samp> command-line parameter: 
    </p>
 
    <p><samp class="codeph"> 
        ./xhpl –n 100000 –b 1024 –p 8 –q 16 –f 
        <em>&lt;heterogeneous factor&gt;</em> -c 
        <em>&lt;number of faster processor columns&gt;</em></samp> 
    </p>
 
    <p>Use both 
      <samp class="codeph">f</samp> and 
      <samp class="codeph">c</samp> command-line parameters together. The 
      <samp class="codeph">-f 1.0 –c 0</samp> setting returns you to the default homogeneous behavior. 
    </p>
 
    <p>To understand how to choose the problem size 
      <var>N</var> for a heterogeneous run, first consider a homogeneous system, where you might choose 
      <var>N</var> as follows: 
    </p>
 
    <p> 
      <var>N</var> ~= 
      <var>sqrt(Memory Utilization * P * Q * Memory Size in GBytes / 8)</var> 
    </p>
 
    <p><var>Memory Utilization</var> is usually around 0.8 for homogeneous Intel Xeon processor systems. With Intel Xeon Phi coprocessors involved, 
      <var>Memory Utilization</var> is probably around 0.7 due to extra buffers needed for communication. 
    </p>
 
    <p>On a heterogeneous system, you might apply a different formula for 
      <var>N</var> for each "cluster" of nodes that are the same and take the minimum 
      <var>N</var> involved. Suppose you have a cluster with only one heterogeneous factor 
      <var>F</var> and the number of processor columns (out of the total 
      <var>Q</var>) in the group with that heterogeneous factor equal to 
      <var>C</var>. That group contains 
      <var>P*C</var> nodes. First compute the sum of the parts: 
      <var>S =F*P*C + P*(Q-C)</var>. Note that 
      <var>S=P*Q,</var> 
      <var>F=</var>1, and 
      <var>C=Q</var> on a homogeneous system. So take 
      <var>N</var> as 
    </p>
 
    <p><var> N ~= sqrt(Memory Utilization * P * Q * ((F*P*C)/S) * Memory Size in GBytes / 8)</var> 
    </p>
 
    <p>or simply scale down the value of 
      <var>N</var> for the homogeneous system by 
      <var>sqrt(F*P*C/S)</var>. 
    </p>
 
    <div class="section" id="GUID-A7D5010D-4B69-4B65-870E-956EBF85F715"><h2 class="sectiontitle">Example</h2> 
       
      <p>Suppose the cluster has 100 nodes each having 64 GB of memory, and 20 of the nodes are 2.7 times as powerful as the other 80. Run 
        <em>one</em> MPI process per node for a total of 100 MPI processes. Assume a square processor grid 
        <var>P=Q=</var>10, which conveniently divides up the faster nodes evenly. Normally, the HPL documentation recommends choosing a matrix size that consumes 80 percent of available memory. If 
        <var>N</var> is the size of the matrix, the matrix consumes 8<var>N</var>^2/(<var>P*Q</var>). So a homogeneous run might look like: 
      </p>
 
      <p><samp class="codeph"> 
          ./xhpl –n 820000 –b 256 –p 10 –q 10</samp> 
      </p>
 
      <p>Unfortunately, the HPL from Netlib runs this problem as slow as the slowest node, and all the extra performance potential from the faster nodes is lost. However, if you redistribute the matrix and run the heterogeneous Intel Optimized MP LINPACK Benchmark, you can take advantage of the faster nodes. But because some of the nodes will contain 2.7 times as much data as the other nodes, you must shrink the problem size (unless the faster nodes also happen to have 2.7 times as much memory). Instead of 0.8*64GB*100 total memory size, we have only 0.8*64GB*20 + 0.8*64GB/2.7*80 total memory, which is less than half the original space. So the problem size in this case would be 526000. If the faster nodes are faster because of the presence of Intel Xeon Phi coprocessors (intra-node heterogeneity), you might need to choose a larger block size as well (which reduces scalability to some extent). Because 
        <var>P</var>=10 and there are 20 faster nodes, two processor columns are faster. If you arrange MPI to send these nodes first to the application, the command line looks like: 
      </p>
 
      <p><samp class="codeph"> 
          ./xhpl –n 526000 –b 1024 –p 10 –q 10 –f 2.7 –c 2</samp> 
      </p>
 
    </div>
 
    <p>The 
      <samp class="codeph">m</samp> parameter may be misleading for heterogeneous calculations, because it calculates the problem size assuming all the nodes have the same amount of data. 
    </p>
 
    <div class="Note"><h3 class="NoteTipHead">Warning</h3> 
      <p>The number of faster nodes must be 
        <var>C</var>*<var>P</var>. If the number of faster nodes is not divisible by 
        <var>P</var>, you might not be able to take advantage of the extra performance potential by giving the faster nodes extra work. 
      </p>
 
    </div> 
    <p>While it suffices to provide 
      <samp class="codeph">f</samp> and 
      <samp class="codeph">c</samp> command-line parameters if you need one heterogeneous factor, to support multiple heterogeneous factors, you must add lines to the 
      <span class="filepath">HPL.dat</span> input as explained below. 
    </p>
 
    <p>For example, if there are three different types of nodes in a cluster and you need at least two heterogeneous factors: 
    </p>
 
    <ol id="GUID-2E521703-2066-48F1-A5F2-F116AFAA752E"> 
      <li> 
        <p>Add these lines to the end of the 
          <span class="filepath">HPL.dat</span>: 
        </p>
 
        <pre>1	      number of heterogeneous factors
0 3 2.7   [start_column, stop_column, heterogeneous factor for that range]
</pre> 
        <div class="Note"><h3 class="NoteTipHead">Note</h3> 
          <p>Numbering of processor columns starts at 0. The start and stopping numbers must be between 0 and 
            <var>Q</var>-1 (inclusive). 
          </p>
 
        </div> 
        <p>You can also perform this step instead of providing the command-line parameters if you need only one heterogeneous factor. 
        </p>
 
      </li>
 
      <li> 
        <p>For two heterogeneous factors, change the number in the first row above from 1 to 2 and follow that line with two lines specifying the start column, stopping column, and heterogeneous factor. 
        </p>
 
      </li>
 
    </ol>
 
    <p>When choosing parameters for heterogeneous support in 
      <span class="filepath">HPL.dat</span>, primarily focus on the most powerful nodes. However, the larger the heterogeneous factor, the more balanced the cluster may be from a performance viewpoint, but the more imbalanced from a memory viewpoint. At some point, further performance balancing might affect the memory too much. If this is the case, try to reduce any changes done for the faster nodes (such as in block sizes). Experiment with values in 
      <span class="filepath">HPL.dat</span> carefully because wrong values may greatly hinder performance. 
    </p>
 
    <p>When tuning on a heterogeneous cluster, do not immediately attempt a heterogeneous run, but do the following: 
    </p>
 
    <ol id="GUID-5B7E8902-3668-46F5-A953-2408F521462B"> 
      <li> 
        <p>Break the cluster down into multiple homogeneous clusters. 
        </p>
 
      </li>
 
      <li> 
        <p>Make heterogeneous adjustments for performance balancing. For instance, if you have two different sets of nodes where one is three times as powerful as the other, it must do three times the work. 
        </p>
 
      </li>
 
      <li> 
        <p>Figure out the approximate size of the problem (per node) that you can run on each piece. 
        </p>
 
      </li>
 
      <li> 
        <p>Do some homogeneous runs with those problem sizes per node and the final block size needed for the heterogeneous run and find the best parameters. 
        </p>
 
      </li>
 
      <li> 
        <p>Use these parameters for an initial heterogeneous run. 
        </p>
 
      </li>
 
    </ol>
 
    <p id="P_CF_12894904934934"><a name="P_CF_12894904934934"><!-- --></a> 
	 
<div class="tablenoborder"><a name="d20e18"><!-- --></a><table cellpadding="4" summary="" id="d20e18" frame="border" border="1" cellspacing="0" rules="all"> 
		  <thead align="left"> 
			 <tr> 
				<th class="cellrowborder" align="left" valign="top" width="100%" id="d66859e380"> 
				  <p id="d20e29"><a name="d20e29"><!-- --></a>Optimization Notice 
				  </p>
 
				</th>
 
			 </tr>
</thead>
 
		  <tbody> 
			 <tr> 
				<td class="bgcolor(#f5f5f5)" bgcolor="#f5f5f5" valign="top" width="100%" headers="d66859e380 "> 
				  <p>Intel's compilers may or may not optimize to the same degree for non-Intel microprocessors for optimizations that are not unique to Intel microprocessors. These optimizations include SSE2, SSE3, and SSSE3 instruction sets and other optimizations. Intel does not guarantee the availability, functionality, or effectiveness of any optimization on microprocessors not manufactured by Intel. Microprocessor-dependent optimizations in this product are intended for use with Intel microprocessors. Certain optimizations not specific to Intel microarchitecture are reserved for Intel microprocessors. Please refer to the applicable product User and Reference Guides for more information regarding the specific instruction sets covered by this notice. 
				  </p>
 
				  <p> Notice revision #20110804 
				  </p>
  

				  </td>
 
			 </tr>
 
		  </tbody>
 
		</table>
</div>
 
	 </p>
 
  </div>
 
  
<div class="familylinks">
<div class="parentlink"><strong>Parent topic:</strong>&nbsp;<a href="GUID-10DFCB17-3953-47C6-9971-8C455A925BFE.htm">Intel&reg; Optimized MP LINPACK Benchmark for Clusters</a></div>
</div>
<div><br clear="all">
<div class="docfeedback">
<div><a href="http://www.intel.com/software/products/softwaredocs_feedback" target="_blank">Submit feedback on this help topic 
		  </a></div></div></div> 

</body>
</html>
